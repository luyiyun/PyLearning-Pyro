{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitb8b279b2598c4fc782452ea1b2872f13",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELBO Gradient Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一部分是来告诉我们`pyro`是如何估计ELBO的梯度的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "∇θ,ϕELBO=∇θ,ϕEqϕ(z)[logpθ(x,z)−logqϕ(z)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单情况：Reparameterizable Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实我们可以从上面的公式中看到，最麻烦的地方是求期望这个部分会存在需要计算梯度的参数phi。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们能够进行下面的改变：\n",
    "\n",
    "$$\\mathbb{E}_{q_{\\phi}(Z)}[f_{\\phi}(Z)]=\\mathbb{E}_{q(\\epsilon)}[f_{\\phi}(g_{\\phi}(\\epsilon))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即将求期望里面的可训练参数去掉，$q(\\epsilon)$是一个确定的分布。则可以进行如下的计算："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\bigtriangledown_{\\phi}{\\mathbb{E}_{q(\\epsilon)}[f_{\\phi}(g_{\\phi}(\\epsilon))]}=\\mathbb{E}_{q(\\epsilon)}[\\bigtriangledown_{\\phi}{f_{\\phi}(g_{\\phi}(\\epsilon))}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用简单的Monte Carlo估计即可。在这种情况下，model只需要能够计算pdf，guide能够进行采样即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 复杂情况：Non-reparameterizable Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种情况也是大部分存在的情况，比如所有的离散分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们进行下面的变化："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bigtriangledown_{\\phi}{\\mathbb{E}_{q_{\\phi}(Z)}[f_{\\phi}(Z)]}=\\bigtriangledown_{\\phi}{\\int{dZq_{\\phi}(Z)f_{\\phi}(Z)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后交换求导和积分（暂且先不去考虑是否可交换），根据求导的规则，有下面的结果："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\int{dZ\\{(\\bigtriangledown_{\\phi}{q_{\\phi}(Z)})f_{\\phi}(Z)+q_{\\phi}(Z)(\\bigtriangledown_{\\phi}{f_{\\phi}(Z)})\\}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用下面的等式：\n",
    "\n",
    "$$\\bigtriangledown_{\\phi}{q_{\\phi}(Z)}=q_{\\phi}(Z)\\bigtriangledown_{\\phi}{\\log{q_{\\phi}(Z)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将进一步得到：\n",
    "\n",
    "$$\\bigtriangledown_{\\phi}{\\mathbb{E}_{q_{\\phi}(Z)}[f_{\\phi}(Z)]}=\\mathbb{E}_{q_{\\phi}(Z)}[(\\bigtriangledown_{\\phi}{\\log{q_{\\phi}(Z)}})f_{\\phi}(Z)+\\bigtriangledown_{\\phi}{f_{\\phi}(Z)}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上的梯度估计也被称为**REINFORCE估计**，可以借助Monte Carlo来进行计算。这里进一步提示：model需要能够计算pdf即可，而guide除了能够计算pdf还需要能够进行采样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，被求梯度的项：\n",
    "\n",
    "$${\\log{q_{\\phi}(Z)}}\\overline{f_{\\phi}(Z)}+f_{\\phi}(Z)$$\n",
    "\n",
    "称为surrogate objective。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但实际上我们不能直接使用上面估计，因为实践中显示这个估计的方差实在是太大了，从而根本无法进行有效的工作。所以我们必须积极寻求降低此估计方差的办法，注意有以下两种测量："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一种，**通过依赖结构**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们有下面的cost function的形式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f_{\\phi}(Z)=\\log{p_{\\theta}(X|Pa_p(X))}+\\sum_i{\\log{p_{\\theta}(Z_i|Pa_p(Z_i))}}-\\sum_i{\\log{q_{\\phi}(Z_i|Pa_q(Z_i))}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再经过一系列的推导，其会将一些项给删去，从而降低了方差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "是实际使用中，只需要将`TraceGraph_ELBO`替换`Trace_ELBO`即可。但注意，这个会进行额外的计算，所以只在有无法进行重参数化技巧的问题上使用，正常使用`Trace_ELBO`即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二种，**通过数据依赖基准（Data-Dependent Baselines）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本思路和上面一致，但并不是去减少一些期望为0的项，而是添加一些特殊项从而减小方差。添加的那一项（一般为常数）就是Baselines。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般的形式是：\n",
    "\n",
    "$${\\log{q_{\\phi}(Z)}}(\\overline{f_{\\phi}(Z)}-b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在pyro中，我们需要为每个以Z设置baseline，所以需要在`pyro.sample`上设置，其拥有参数`infer`，其接受dict，其中一个key可以是`baseline`，然后其value是dict，包括一些真正关于baseline的设置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "z = pyro.sample(\"z\", dist.Bernoulli(...),\n",
    "                infer=dict(baseline={'use_decaying_avg_baseline': True,\n",
    "                                     'baseline_beta': 0.95}))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这还是那个coins反转的例子，只是这里不再将Beta分布视为可以重参数化的分布，而是使用baselines技巧来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Doing inference with use_decaying_avg_baseline=True\n..\nDid 167 steps of inference.\nFinal absolute errors for the two variational parameters were 0.7943 & 0.7986\nDoing inference with use_decaying_avg_baseline=False\n.........\nDid 878 steps of inference.\nFinal absolute errors for the two variational parameters were 0.7978 & 0.7138\n"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.distributions.constraints as constraints\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.distributions.testing.fakes import NonreparameterizedBeta\n",
    "import pyro.optim as optim\n",
    "from pyro.infer import SVI, TraceGraph_ELBO\n",
    "\n",
    "def param_abs_error(name, target):\n",
    "    return torch.sum(torch.abs(target - pyro.param(name))).item()\n",
    "\n",
    "class BernoulliBetaExample:\n",
    "    def __init__(self, max_steps):\n",
    "        # the maximum number of inference steps we do\n",
    "        self.max_steps = max_steps\n",
    "        # the two hyperparameters for the beta prior\n",
    "        self.alpha0 = 10.0\n",
    "        self.beta0 = 10.0\n",
    "        # the dataset consists of six 1s and four 0s\n",
    "        self.data = torch.zeros(10)\n",
    "        self.data[0:6] = torch.ones(6)\n",
    "        self.n_data = self.data.size(0)\n",
    "        # compute the alpha parameter of the exact beta posterior\n",
    "        self.alpha_n = self.data.sum() + self.alpha0\n",
    "        # compute the beta parameter of the exact beta posterior\n",
    "        self.beta_n = - self.data.sum() + torch.tensor(self.beta0 + self.n_data)\n",
    "        # initial values of the two variational parameters\n",
    "        self.alpha_q_0 = 15.0\n",
    "        self.beta_q_0 = 15.0\n",
    "\n",
    "    def model(self, use_decaying_avg_baseline):\n",
    "        # sample `latent_fairness` from the beta prior\n",
    "        f = pyro.sample(\"latent_fairness\", dist.Beta(self.alpha0, self.beta0))\n",
    "        # use plate to indicate that the observations are\n",
    "        # conditionally independent given f and get vectorization\n",
    "        with pyro.plate(\"data_plate\"):\n",
    "            # observe all ten datapoints using the bernoulli likelihood\n",
    "            pyro.sample(\"obs\", dist.Bernoulli(f), obs=self.data)\n",
    "\n",
    "    def guide(self, use_decaying_avg_baseline):\n",
    "        # register the two variational parameters with pyro\n",
    "        alpha_q = pyro.param(\"alpha_q\", torch.tensor(self.alpha_q_0),\n",
    "                             constraint=constraints.positive)\n",
    "        beta_q = pyro.param(\"beta_q\", torch.tensor(self.beta_q_0),\n",
    "                            constraint=constraints.positive)\n",
    "        # sample f from the beta variational distribution\n",
    "        baseline_dict = {'use_decaying_avg_baseline': use_decaying_avg_baseline,\n",
    "                         'baseline_beta': 0.90}\n",
    "        # note that the baseline_dict specifies whether we're using\n",
    "        # decaying average baselines or not\n",
    "        pyro.sample(\"latent_fairness\", NonreparameterizedBeta(alpha_q, beta_q),\n",
    "                    infer=dict(baseline=baseline_dict))\n",
    "\n",
    "    def do_inference(self, use_decaying_avg_baseline, tolerance=0.80):\n",
    "        # clear the param store in case we're in a REPL\n",
    "        pyro.clear_param_store()\n",
    "        # setup the optimizer and the inference algorithm\n",
    "        optimizer = optim.Adam({\"lr\": .0005, \"betas\": (0.93, 0.999)})\n",
    "        svi = SVI(self.model, self.guide, optimizer, loss=TraceGraph_ELBO())\n",
    "        print(\"Doing inference with use_decaying_avg_baseline=%s\" % use_decaying_avg_baseline)\n",
    "\n",
    "        # do up to this many steps of inference\n",
    "        for k in range(self.max_steps):\n",
    "            svi.step(use_decaying_avg_baseline)\n",
    "            if k % 100 == 0:\n",
    "                print('.', end='')\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            # compute the distance to the parameters of the true posterior\n",
    "            alpha_error = param_abs_error(\"alpha_q\", self.alpha_n)\n",
    "            beta_error = param_abs_error(\"beta_q\", self.beta_n)\n",
    "\n",
    "            # stop inference early if we're close to the true posterior\n",
    "            if alpha_error < tolerance and beta_error < tolerance:\n",
    "                break\n",
    "\n",
    "        print(\"\\nDid %d steps of inference.\" % k)\n",
    "        print((\"Final absolute errors for the two variational parameters \" +\n",
    "               \"were %.4f & %.4f\") % (alpha_error, beta_error))\n",
    "\n",
    "# enable validation (e.g. validate parameters of distributions)\n",
    "assert pyro.__version__.startswith('1.3.0')\n",
    "pyro.enable_validation(True)\n",
    "\n",
    "# this is for running the notebook in our testing framework\n",
    "smoke_test = ('CI' in os.environ)\n",
    "max_steps = 2 if smoke_test else 10000\n",
    "bbe = BernoulliBetaExample(max_steps=max_steps)\n",
    "bbe.do_inference(use_decaying_avg_baseline=True)\n",
    "bbe.do_inference(use_decaying_avg_baseline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述结果显示，baselines的加入使得我们训练的steps减少了许多，提高了效率。"
   ]
  }
 ]
}