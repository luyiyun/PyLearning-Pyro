{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitb8b279b2598c4fc782452ea1b2872f13",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor shapes in Pyro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一部分主要介绍pyro是如何组织tensor的dimensions的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一个分布，其有两个shapes：即batch_shape和event_shape。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro.distributions as pdist\n",
    "import torch.distributions as tdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgaussion = pdist.Normal(0, 1)\n",
    "tgaussion = tdist.Normal(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "batch_shape: torch.Size([]) torch.Size([])\nevent_shape: torch.Size([]) torch.Size([])\n"
    }
   ],
   "source": [
    "# 注意，隔离pyro.distributions和torch.distributions行为是一致的\n",
    "print(\"batch_shape:\", pgaussion.batch_shape, tgaussion.batch_shape)\n",
    "print(\"event_shape:\", pgaussion.event_shape, tgaussion.event_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pgaussion.sample()\n",
    "assert x.shape == pgaussion.batch_shape + pgaussion.event_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([1, 2, 3])"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "# 注意，torch.Size对象的`__add__`方法是concat到一起而不是加在一起，其类似list\n",
    "torch.Size([1, 2]) + torch.Size([3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上，我们还有另外一个shape：samle_shape，其在使用`sample`方法的时候出现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = pgaussion.sample([3])\n",
    "assert x2.shape == torch.Size([3]) + pgaussion.batch_shape + pgaussion.event_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以我们得到的样本的shape和以上3个shape间的关系是：\n",
    "```\n",
    "      |      iid     | independent | dependent\n",
    "------+--------------+-------------+------------\n",
    "shape = sample_shape + batch_shape + event_shape\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们来具体介绍这3个shapes：\n",
    "\n",
    "- `sample_shape`表示的是样本的维度，比如`sample_shape=[3, 4]`表示我们一共随机了产生了12个独立的样本，然后把每一个样本排列成$3\\times4$的矩阵形式。\n",
    "- `batch_shape`表示的是参数的维度（这里说是independent的维度，也是合理的），我们可以设置参数是`torch.tensor([2, 3])`，则`batch_shape=[2]`，也就是说会产生2个分布，这两个分布是相互独立的，其中一个分布的参数是2，另一个分布的参数是3。\n",
    "- `event_shape`表示的是多维分布的那个\"多维\"，比如多维正态分布、Categorical distribution等等，这些维度上的各个值间不是独立的，因为会收到其他值的影响（当然这是多维正态分布的协方差不能是0）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：\n",
    "\n",
    "1. `log_prob`函数只会考虑到sample_shape和batch_shape，不管多少维的随机变量，其密度函数只会映射到$[0, 1]$区间中。\n",
    "2. 一般来说，`torch.Size([])`和`torch.Size([1])`是不一样的，对于`Normal`，其`event_shape==torch.Size([])`，表示其作为一个单维度的随机变量没有`event_shape`，而对于`MultivariateNormal`，虽然我们也可以让其只有一维，但`event_shape==torch.Size([])`，表示其是一个多维随机变量，什么时候都有`event_shape`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是一些例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.Size()对象`__equal__`方法可以对tuple进行操作\n",
    "d = pdist.Bernoulli(0.5)\n",
    "assert d.batch_shape == ()\n",
    "assert d.event_shape == ()\n",
    "x = d.sample()\n",
    "assert x.shape == ()\n",
    "assert d.log_prob(x).shape == ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pdist.Bernoulli(0.5 * torch.ones(3, 4))\n",
    "assert d.batch_shape == (3, 4)\n",
    "assert d.event_shape == ()\n",
    "x = d.sample()\n",
    "assert x.shape == (3, 4)\n",
    "assert d.log_prob(x).shape == (3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 还可以通过`expand()`方法来形成batch_shape，此方法的效果是在当前batch_shape的基础上进一步扩增batch_shape\n",
    "d = pdist.Bernoulli(torch.tensor([0.1, 0.2, 0.3, 0.4])).expand([3, 4])\n",
    "assert d.batch_shape == (3, 4)\n",
    "assert d.event_shape == ()\n",
    "x = d.sample()\n",
    "assert x.shape == (3, 4)\n",
    "assert d.log_prob(x).shape == (3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate distribution有nonempty的`.event_shape`\n",
    "d = pdist.MultivariateNormal(torch.zeros(3), torch.eye(3, 3))\n",
    "assert d.batch_shape == ()\n",
    "assert d.event_shape == (3,)\n",
    "x = d.sample()\n",
    "assert x.shape == (3,)\n",
    "assert d.log_prob(x).shape == ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is nonempty sample_size，then...\n",
    "x2 = d.sample([4])\n",
    "assert x2.shape == (4, 3)\n",
    "assert d.log_prob(x2).shape == (4,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在pyro中我们可以有方法`.to_event(n)`来把某个`batch_shape`转变成`event_shape`，也就是把一些独立的分布看做是一个联合分布。这在torch中没有。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pdist.Bernoulli(0.5 * torch.ones(3, 4)).to_event(1)\n",
    "assert d.batch_shape == (3,)\n",
    "assert d.event_shape == (4,)\n",
    "x = d.sample()\n",
    "assert x.shape == (3, 4)\n",
    "assert d.log_prob(x).shape == (3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个转换必须从右边开始，里面的参数不是第几个，而是一共要转换几个维度\n",
    "d = pdist.Bernoulli(0.5 * torch.ones(3, 4)).to_event(2)\n",
    "assert d.batch_shape == ()\n",
    "assert d.event_shape == (3, 4)\n",
    "x = d.sample()\n",
    "assert x.shape == (3, 4)\n",
    "assert d.log_prob(x).shape == ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将多个独立的分布看成一个整个的联合分布，这个操作在pyro中会非常频繁的使用。其有两个优势：\n",
    "\n",
    "1. 这使得我们可以简单的构建一个多维分布。\n",
    "2. 这可以使我们避免去使用`plate`来声明一个Multivariate distribution的独立性。\n",
    "\n",
    "（当然，这并不一定是个好处，因为被`plate`声明了独立性后，在进行梯度估计的时候可以受益于这个独立性，而使用`to_event()`方式构建的多维独立分布在pyro的眼中并不是独立的，pyro将其作为dependent variables来看待。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一个版本\n",
    "import pyro\n",
    "\n",
    "x = pyro.sample(\"x\", pdist.Normal(0, 1).expand([10]).to_event(1))\n",
    "assert x.shape == (10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二个版本\n",
    "with pyro.plate(\"x_plate\", 10):\n",
    "    x = pyro.sample(\"x\", pdist.Normal(0, 1))\n",
    "    assert x.shape == (10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，将independent variables看做是dependent，永远都是safe的（特别是对于那些可以reparametric的distributions，此trick使得即使增加了dependence也不会降低效率）。而将dependent variables看做是independent则会导致一些问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用`plate`来declaring independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以使用`plate`的上下文管理器来指定特定的batch dimensions是independence。这些independence可能会被inference algorithm利用从而降低Enforce estimation的variance。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，虽然plate可以操作某个batch dimension是independent，但其并没有将该dimension放入到event_shape中。真正将dimension放入event_shape中的操作是`to_event`，但其并没有指定independent（尽管其实就是independent）。那我们之所以需要使用`plate`来进行操作，只是为了能够利用到其inference时的trick而已。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有几种利用`plate`的方式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一，最简单的，直接在`sample`上使用，则其最右边的batch dimension被设定为independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-0.3545,  2.2652,  1.9722,  2.2458],\n        [ 3.5141,  4.9207,  6.3201,  8.1640],\n        [ 9.1618,  9.5082,  9.0513, 10.9938]])\n"
    }
   ],
   "source": [
    "with pyro.plate(\"my_plate\"):\n",
    "    ss = pyro.sample(\"x\", pdist.Normal(torch.arange(12).reshape(3, 4).float(), 1.))\n",
    "    print(ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二，我们提供最右边的batch dimension在`plate`中作为一个参数，其效果和上面的一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 1.1461,  0.6381,  2.7226,  3.7322],\n        [ 5.3836,  5.0731,  6.4878,  6.5001],\n        [ 7.5291,  9.6671,  9.0649, 10.6666]])\n"
    }
   ],
   "source": [
    "with pyro.plate(\"my_late\", 4):\n",
    "    ss = pyro.sample(\"x\", pdist.Normal(torch.arange(12).reshape(3, 4).float(), 1.))\n",
    "    print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([1.8814, 1.2337])\n"
    }
   ],
   "source": [
    "with pyro.plate(\"my_late\", 2):\n",
    "    ss = pyro.sample(\"x\", pdist.Normal(torch.tensor([1.0, 2.0]), 1.))\n",
    "    print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([1.2504, 1.0534])\n"
    }
   ],
   "source": [
    "# 这种方法的优势1：如果只有一个batch dimension，而且就想让这个batch dimension是independent，而且这个dimension就是多个distribution的\n",
    "#   简单重复，则可以不需要在下面的`sample`语句中指定这个dimension\n",
    "with pyro.plate(\"my_late\", 2):\n",
    "    ss = pyro.sample(\"x\", pdist.Normal(1.0, 1.0))\n",
    "    print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[0.5843, 2.2563, 0.0241],\n         [2.7450, 3.8366, 5.4417]]])\n"
    }
   ],
   "source": [
    "# 这种方法的优势2：可以通过嵌套的`plate`上下文管理器，来实现多个维度的independent\n",
    "with pyro.plate(\"x_axis\", 3):  # 注意，这是第-1个dimension\n",
    "    with pyro.plate(\"y_axis\", 2):  # 注意，这是第-2个dimension\n",
    "        ss = pyro.sample(\"x\", pdist.Normal(torch.arange(6).reshape(1, 2, 3).float(), 1.))\n",
    "        print(ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三，使用`plate`中的参数`dim`来指定哪一个维度是independent，这使得我们可以通过合理的组织来完成复杂的依赖关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1():\n",
    "    a = pyro.sample(\"a\", pdist.Normal(0, 1))\n",
    "    b = pyro.sample(\"b\", pdist.Normal(torch.zeros(2), 1).to_event(1))\n",
    "    with pyro.plate(\"c_plate\", 2):\n",
    "        c = pyro.sample(\"c\", pdist.Normal(torch.zeros(2), 1))\n",
    "    with pyro.plate(\"d_plate\", 3):\n",
    "        d = pyro.sample(\"d\", pdist.Normal(torch.zeros(3, 4, 5), 1).to_event(2))\n",
    "    assert a.shape == ()           # batch_shape == (), event_shape == (), pyro independ == (), real independ == ()\n",
    "    assert b.shape == (2,)         # batch_shape == (), event_shape == (2,), pyro independ == (), real independ == (2,)\n",
    "    assert c.shape == (2,)         # batch_shape == (2,), event_shape == (), pyro independ == (2,), real independ == (2,)\n",
    "    assert d.shape == (3, 4, 5)    # batch_shape == (3, ), event_shape == (4, 5), pyro independ == (4, 5), real independ == (3, 4, 5)\n",
    "\n",
    "    x_axis = pyro.plate(\"x_axis\", 3, dim=-2)\n",
    "    y_axis = pyro.plate(\"y_axis\", 2, dim=-3)\n",
    "    with x_axis:\n",
    "        x = pyro.sample(\"x\", pdist.Normal(0, 1))\n",
    "    with y_axis:\n",
    "        y = pyro.sample(\"y\", pdist.Normal(0, 1))\n",
    "    with x_axis, y_axis:\n",
    "        xy = pyro.sample(\"xy\", pdist.Normal(0, 1))\n",
    "        z = pyro.sample(\"z\", pdist.Normal(0, 1).expand([5]).to_event(1))\n",
    "    assert x.shape == (3, 1)         # batch_shape == (3, 1), event_shape == (), pyro independ == (3,), real independ == (3, 1)\n",
    "    assert y.shape == (2, 1, 1)      # batch_shape == (2, 1, 1), event_shape == (), pyro independ == (2,), real independ == (2, 1, 1)\n",
    "    assert xy.shape == (2, 3, 1)     # batch_shape == (2, 3, 1), event_shape == (), pyro independ == (2, 3), real independ == (2, 3, 1)\n",
    "    assert z.shape == (2, 3, 1, 5)   # batch_shape == (2, 3, 1), event_shape == (5,), pyro independ == (2, 3), real independ == (2, 3, 1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, guide, loss):\n",
    "    pyro.clear_param_store()\n",
    "    loss.loss(model, guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import Trace_ELBO\n",
    "\n",
    "test_model(model1, model1, Trace_ELBO())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到，在使用`log_prob`的时候，event_shape会被sum out到一起，剩下的batch_shape（和sample_shape）会保留下来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Trace Shapes:            \n Param Sites:            \nSample Sites:            \n       a dist       |    \n        value       |    \n     log_prob       |    \n       b dist       | 2  \n        value       | 2  \n     log_prob       |    \n c_plate dist       |    \n        value     2 |    \n     log_prob       |    \n       c dist     2 |    \n        value     2 |    \n     log_prob     2 |    \n d_plate dist       |    \n        value     3 |    \n     log_prob       |    \n       d dist     3 | 4 5\n        value     3 | 4 5\n     log_prob     3 |    \n  x_axis dist       |    \n        value     3 |    \n     log_prob       |    \n  y_axis dist       |    \n        value     2 |    \n     log_prob       |    \n       x dist   3 1 |    \n        value   3 1 |    \n     log_prob   3 1 |    \n       y dist 2 1 1 |    \n        value 2 1 1 |    \n     log_prob 2 1 1 |    \n      xy dist 2 3 1 |    \n        value 2 3 1 |    \n     log_prob 2 3 1 |    \n       z dist 2 3 1 | 5  \n        value 2 3 1 | 5  \n     log_prob 2 3 1 |    \n"
    }
   ],
   "source": [
    "import pyro.poutine as poutine\n",
    "\n",
    "trace = poutine.trace(model1).get_trace()\n",
    "trace.compute_log_prob()\n",
    "print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([-5.7662, -6.1560, -6.4239])"
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "d = pdist.Normal(torch.ones(3, 4), 1.0).to_event(1)\n",
    "x = d.sample()\n",
    "d.log_prob(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling tensors inside a `plate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前说到过，`plate`的一个作用是用来做subsampling的。实际上做subsampling是需要一些条件的，比如必须是独立的，不然expected loss of subsamples并不会成比例的减小。所以一般来说subsampling只会用在样本那一层级。\n",
    "\n",
    "但现在因为数据结构的原因，我们使用`plate`将一些维度也规定成independent，则subsampling也是可以使用的。（尽管理论上是这样，但实际上这个subsampling也只会应用到data那里）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用时，我们需要即指定数据的总数量，而且还要指定`subsample_size`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.arange(100.)\n",
    "def model2():\n",
    "    mean = pyro.param(\"mean\", torch.zeros(len(data)))\n",
    "    with pyro.plate(\"data\", len(data), subsample_size=10) as ind:  # subsampling的逻辑需要在整个模型中体现出来\n",
    "        assert len(ind) == 10\n",
    "        batch = data[ind]\n",
    "        mean_batch = mean[ind]\n",
    "        x = pyro.sample(\"x\", pdist.Normal(mean_batch, 1), obs=batch)\n",
    "        assert len(x) == 10\n",
    "\n",
    "test_model(model2, guide=lambda: None, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting to allow parallel enumeration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，pyro可以对一些discrete latent variables进行并行运算，这可以显著的降低gradient estimators’ variance。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而为了能够进行这个parallel enumeration，pyro需要知道哪些tensor dimensions可以被enumerate，所以我们需要指定一个`max_plate_nesting`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import config_enumerate, TraceEnum_ELBO\n",
    "\n",
    "@config_enumerate\n",
    "def model3():\n",
    "    p = pyro.param(\"p\", torch.arange(6.) / 6)\n",
    "    locs = pyro.param(\"locs\", torch.tensor([-1., 1.]))\n",
    "\n",
    "    a = pyro.sample(\"a\", pdist.Categorical(torch.ones(6) / 6))\n",
    "    b = pyro.sample(\"b\", pdist.Bernoulli(p[a]))\n",
    "    with pyro.plate(\"c_plate\", 4):\n",
    "        c = pyro.sample(\"c\", pdist.Bernoulli(0.3))\n",
    "        with pyro.plate(\"d_plate\", 5):\n",
    "            d = pyro.sample(\"d\", pdist.Bernoulli(0.4))\n",
    "            e_loc = locs[d.long()].unsqueeze(-1)\n",
    "            e_scale = torch.arange(1., 8.)\n",
    "            e = pyro.sample(\"e\", pdist.Normal(e_loc, e_scale).to_event(1))\n",
    "\n",
    "    #                   enumerated/batch/event dims\n",
    "    assert a.shape == (         6, 1, 1   )\n",
    "    assert b.shape == (      2, 1, 1, 1   )\n",
    "    assert c.shape == (   2, 1, 1, 1, 1   )\n",
    "    assert d.shape == (2, 1, 1, 1, 1, 1,  )\n",
    "    assert e.shape == (2, 1, 1, 1, 5, 4, 7)\n",
    "\n",
    "    assert e_loc.shape   == (2, 1, 1, 1, 1, 1, 1,)\n",
    "    assert e_scale.shape == (                  7,)\n",
    "\n",
    "test_model(model3, model3, TraceEnum_ELBO(max_plate_nesting=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p "
   ]
  }
 ]
}